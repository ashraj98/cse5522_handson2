{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE5522 Hands-on #2: Naive Bayes for Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashraj98/cse5522_handson2/blob/master/CSE5522_Hands_on_2_Naive_Bayes_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTPwb8ny-hOU",
        "colab_type": "text"
      },
      "source": [
        "**CSE 5522 Hands-on #2: Naive Bayes**\n",
        "\n",
        "The goals of today's exercise are to familarize you with:\n",
        "\n",
        "\n",
        "*   Naive Bayes\n",
        "*   Binary Classification\n",
        "*   Data exploration\n",
        "\n",
        "**END OF CLASS GOAL:** Submit a link to your notebook (Share > Get Sharable Link) in Carmen so I can see how far you got.  This should be submitted in a group assignment page on Carmen.\n",
        "\n",
        "**Part 0: Initial setup**\n",
        "\n",
        "**0.0:** If none of your team members are familar with python this will be difficult to accomplish - you may want to split up and join different groups.\n",
        "\n",
        "**0.1:** Go to the Carmen Page for CSE 5522, and find the Group signup tab.  Choose a group under HandsOn2-xx (where xx=1 to 20), and sign your group members up.  This will allow you to submit a group assignment at the end.\n",
        "\n",
        "**0.2:** Make a copy of this page in your google drive so that you can edit it. Edit the filename to include your group number.  Share the copied page with your teammates. At the end of class, share a URL and submit (so I can see how far you got).  This will count as the participation grade for all members.\n",
        "\n",
        "**0.3:** While not completely necessary for this assignment, you may want to familiarize yourself with the following packages: [numpy](https://numpy.org), [scikit-learn](https://scikit-learn.org), [pandas](https://pandas.pydata.org), [matplotlib](https://matplotlib.org).\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2T5S8IQTct",
        "colab_type": "text"
      },
      "source": [
        "**Part 1: A Simple Bayes Net: Naive Bayes**\n",
        "\n",
        "In class, we discussed how conditional independences of a joint probablity distribution get encoded by a Bayesian Network. One of the simplest form of BNs is the Naive Bayes model which encodes a set of simple conditional independences: \n",
        "\n",
        "- Given a single cause all of the effects are independent from each other.\n",
        "- Mathematically: \n",
        "$P($*cause*$, $*effect*$_1, ..., $*effect*$_n) = P($*cause*$) \\prod_i P($*effect*$_i|$*cause*$)$ \n",
        "\n",
        "NB can be used for classification by assuming that cause is the true (unknown) label and it (probabilistically) generates all of the features (effects) while features are independent given the cause. \n",
        "\n",
        "For example, in sentiment analysis the *cause* is the author's sentiment (say, unknown label from the set of {sad, happy, feared, suprised, disgusted, angry}) and the *effects* are words that s/he writes. The simplifying assumption of NB says that knowing the latent sentiment, words of the sentence are independent. We know this assumption is not true because grammar and word-use impose some dependency structure between words in the sentence, but we choose to ignore that in this model.\n",
        "\n",
        "Although simple, NB has shown good performance in many classifcation tasks and has become a standard classic baseline for classification. \n",
        "\n",
        "Today we want to perform Twitter sentiment analysis using NB. The goal is to figure out if a tweet has a positive or negative sentiment about the weather.  \n",
        "\n",
        "**1.0:** Set up the environment (you can click on the play button below to import the appropriate modules)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6fYsm5f-UbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-MaOhV5UUvD",
        "colab_type": "text"
      },
      "source": [
        "**1.1** Read the data from GitHub into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HskImt85UhU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TweetUrl='https://github.com/aasiaeet/cse5522data/raw/master/db3_final_clean.csv'\n",
        "tweet_dataframe=pd.read_csv(TweetUrl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvgt0PYLVHYr",
        "colab_type": "text"
      },
      "source": [
        "**1.2** Print out the top of the dataframe to make sure that the data loaded correctly.  It should be a data table with three columns (weight, tweet, label), and 3697 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScERznWSVBK3",
        "colab_type": "code",
        "outputId": "f583a0c5-8ba2-4b20-bab3-02efa1d3def8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "display(tweet_dataframe.shape)\n",
        "tweet_dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3697, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weight</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>it is very cold out want it to be warmer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7698</td>\n",
              "      <td>dammmmmmm its pretty cold this morning   burr lol</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6146</td>\n",
              "      <td>why does halsey have to be so far away think m...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9356</td>\n",
              "      <td>dammit stop being so cold so can work out</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>its too freakin cold</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weight                                              tweet  label\n",
              "0  1.0000          it is very cold out want it to be warmer      -1\n",
              "1  0.7698  dammmmmmm its pretty cold this morning   burr lol     -1\n",
              "2  0.6146  why does halsey have to be so far away think m...     -1\n",
              "3  0.9356         dammit stop being so cold so can work out      -1\n",
              "4  1.0000                               its too freakin cold     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLwuLvFQuvGU",
        "colab_type": "text"
      },
      "source": [
        "Labels are -1 and +1 for negative and positive sentiments respectively. Multiple judges have been asked to choose a label for a tweet (this is an example of crowd-sourcing) from five possible labels: \n",
        "\n",
        "- Tweet is not relevant to weather. \n",
        "- I can't tell the sentiment. \n",
        "- Neutral: author just sharing information. \n",
        "- Positive\n",
        "- Negative\n",
        "\n",
        "The majority vote was picked as the label and its ratio was set as the weight of the tweet. So for the tweet in row 2 above, 61% of judges voted that the label is negative.\n",
        "\n",
        "Note that tweets have been pre-processed (or cleaned). For example, :) and :( :) were replaced with \"sad\" and \"smiley\" and numbers with \"num\", etc. You can go further (as we ask in 1.12) and remove the stop words, i.e., repetitive non-informative words such as am, is, and are. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AssaBgdR0K",
        "colab_type": "text"
      },
      "source": [
        "**1.3.** In the next step, we should build our feature matrix by converting the string of words to a vector of numeric values. \n",
        "\n",
        "First we need to assign a unique id to each word and create the feature matrix with correct size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Q7tGhlVcOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordDict maps words to id\n",
        "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
        "wordDict = {}\n",
        "idCounter = 0\n",
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    if word not in wordDict:\n",
        "      wordDict[word] = idCounter\n",
        "      idCounter += 1\n",
        "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JkM_oBZ7cv7",
        "colab_type": "text"
      },
      "source": [
        "Checking head of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdK4g_D8hX-4",
        "colab": {}
      },
      "source": [
        "dict(list(wordDict.items())[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmXPr6qeSQo",
        "colab_type": "text"
      },
      "source": [
        "**1.4:** The simplest way of coding a tweet to numbers is to mark the occurrence of a word, and forget about its frequency in the document (tweet). This works well with tweets as there are not many repetitive words in a single tweet. So let's fill the document-word matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap5o8fzI7rgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    X[i, wordDict[word]]  = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "714iX2JA9XMh",
        "colab_type": "text"
      },
      "source": [
        "Now we check if the number of words are correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiap5wBW86lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sum(X[0:5, ], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmPNK1Su-Hwf",
        "colab_type": "text"
      },
      "source": [
        "Finally, we extract the labels from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGm_x8Nm-HL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(tweet_dataframe.iloc[:,2])\n",
        "y[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ-EgzBo-wLd",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the total number of positive and negative tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFKKNsM7-_UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numNeg = np.where(y > 0)[0][0] - 1\n",
        "numPos = len(y) - numNeg\n",
        "probNeg = numNeg / (numNeg + numPos)\n",
        "probPos = 1 - probNeg\n",
        "display(numNeg, numPos, probNeg, probPos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlOIWDiI7JQw",
        "colab_type": "text"
      },
      "source": [
        "So samples 0:1649 are negative and 1650:-1 are positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-s3N0xJOILK",
        "colab_type": "text"
      },
      "source": [
        "**1.5: Train/Test Split** Now with do the 20/80 split and learn the word probabilities using the 80 % part and test the NB performance on the 20 % part. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbdLXcY0PCQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "display(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2LTz2CP95GT",
        "colab_type": "text"
      },
      "source": [
        "**1.6: Computing Probabilities by Counting** Now the real work begins. Write the code that, from the train feature matrix xTrain computes the needed word probabilites, i.e., $P(word|label)$ where label is + or - and word is any of the words saved in the `wordDict`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ7KjLPk7oMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute three distributions (four variables):\n",
        "#\n",
        "# probWordGivenPositive: P(word|Sentiment = +ive)\n",
        "# probWordGivenNegative: P(word|Sentiment = -ive)\n",
        "# priorPositive: P(Sentiment = +ive)\n",
        "# priorNegative: P(Sentiment = -ive)\n",
        "#  (note these last two form one distribution)\n",
        "\n",
        "# compute distributions here\n",
        "\n",
        "\n",
        "# checking the results\n",
        "display(probWordGivenPositive[0:5])\n",
        "display(probWordGivenNegative[0:5])\n",
        "display(priorPositive, priorNegative)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUFU8eUQ_8gC",
        "colab_type": "text"
      },
      "source": [
        "Note that you only needed to compute $P(word = 1| +)$ or $P(word = 1| -)$ and the probabilities of the word being absent from a tweet is just 1 minus those probabilities. \n",
        "\n",
        "However, as we see in 1.6, for convenience, we will also want to compute $log P(word = 1 | +)$, $log P(word = 0 | +)$, $log P(word = 1 | -)$ and $log P(word = 0 | -)$.  Also we should compute the log priors.  Let's do so now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLcaaDTiwF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute the following:\n",
        "# logProbWordPresentGivenPositive\n",
        "# logProbWordAbsentGivenPositive\n",
        "# logProbWordPresentGivenNegative\n",
        "# logProbWordAbsentGivenNegative\n",
        "# logPriorPositive\n",
        "# logPriorNegative\n",
        "\n",
        "\n",
        "# Did this work, or did you get an error?  (Read below.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXVQ7ZHAkH1u",
        "colab_type": "text"
      },
      "source": [
        "You likely received an error when you tried to take $log(0)$ at some point.  Can your group think of a way to avoid taking $log(0)$?  Check in with your instructor/TA to see if what you're thinking will work.  Implement that change in your code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxdbYsu9E8av",
        "colab_type": "text"
      },
      "source": [
        "**1.6: Math of NB** Here we provide the derivation of NB when we want to classify the $i$th tweet $\\textbf{x}^{(i)}$ and the size of dictionary is $p$, i.e., each tweet is a binary vector of size $p$ as $\\textbf{x}^{(i)} = (x_1^{(i)},\\dots, x_p^{(i)})$. \n",
        "\n",
        "Note that we computed $P(x_j^{(i)} = 1|+)$ and $P(x_j^{(i)} = 1|-)$ in above code from `xTrain` and now want to classify `xTest` samples.\n",
        "\n",
        "**Classification Rule:** For each tweet $i$ NB classifier assigns label + if $P(+|\\textbf{x}^{(i)}) > P(-|\\textbf{x}^{(i)})$ and negative otherwise. \n",
        "\n",
        "These posterior probabilities can be computed using prior probabilities (that we got from `xTrain`) and Bayes rule as follows: \n",
        "\n",
        "\\begin{align}\n",
        "P(+|\\textbf{x}^{(i)}) &= \\alpha P(\\{\\textbf{x}^{(i)}\\}_{i=1}^n | +)P(+) \n",
        "\\\\\n",
        "(\\text{NB Assumption}) &= \\alpha P(+) \\prod_{j=1}^p P(x_j^{(i)}|+)\n",
        "\\end{align}\n",
        "\n",
        "For computational convinence (preventing underflow while dealing with small numbers) we work with the $\\log$ of probabilities:\n",
        "\n",
        "\\begin{align} \n",
        "\\log(P(+|\\textbf{x}^{(i)})) &\\propto \\log P(+) + \\sum_{j=1}^p \\log P(x_j^{(i)}|+) \n",
        "\\\\\n",
        "\\log(P(-|\\textbf{x}^{(i)})) &\\propto \\log P(-) + \\sum_{j=1}^p \\log P(x_j^{(i)}|-) \n",
        "\\end{align} \n",
        "\n",
        "Finally we can compute the confidence of our prediction as the log of the ratio of posteriors: \n",
        "$\\log(\\frac{P(\\text{predicted label}|\\textbf{x}^{(i)})}{P(\\text{the other label}|\\textbf{x}^{(i)})})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1AvG-LXTmPJ",
        "colab_type": "text"
      },
      "source": [
        "**1.7: Implementing NB** Now write a function that takes a row of `xTest` and output a label for it based on NB classification rule. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu3YKPlzeFLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classifyNB: \n",
        "#   words - vector of words of the tweet (binary vector)\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns (label of x according to the NB classification rule, confidence about the label)\n",
        "\n",
        "# Note: you can also change the function definition if you wish to encapsulate all six log probs\n",
        "# as one model; just make sure to follow through below\n",
        "\n",
        "def classifyNB(words,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "               logPriorPositive, logPriorNegative):\n",
        "  # fill in function definition here\n",
        "  \n",
        "print(classifyNB(xTest[700, ], fromPositive,fromNegative, probNegTr, probPosTr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7F8osLy3ia",
        "colab_type": "text"
      },
      "source": [
        "**1.8:** Compute the output of `classifyNB` for all test data and output the average error.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_J3BdyCfCVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testNB: Classify all xTest\n",
        "#   xTest - test data features\n",
        "#   yTest - true label of test data\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns Average test error\n",
        "def testNB(xTest, yTest, \n",
        "           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "           logPriorPositive, logPriorNegative):\n",
        "\n",
        "  # compute avgErr\n",
        "  \n",
        "  print(\"Average error of NB is\", avgErr)\n",
        "  return avgErr\n",
        "\n",
        "testNB(xTest, yTest, \n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "       logPriorPositive, logPriorNegative)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJ97nqVz8CW",
        "colab_type": "text"
      },
      "source": [
        "**1.9:** Now write an outer wrapper that perform 10 train/test split and compute the mean and standard deviation of the average error across 10 runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNGxLT9qzOXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10 train/test splits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InZwM9Fv04R5",
        "colab_type": "text"
      },
      "source": [
        "**1.10:** Compare your results against a known NB algorithm from *scikit-learn*.  Do you get the same average error? (Run the 80/20 split 10 times using the code below as a template)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VauBe0gZ029E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import # pick a module here\n",
        "# fit the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR81RBzh2AW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10 train/test splits using sklearn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8giG8HggnNOO",
        "colab_type": "text"
      },
      "source": [
        "**1.11: Incorporating weights of samples** How can we incorporate weight of the labels in our NB algorithm? Revise the way you compute probabilities to incorporate the weights and re-run the final test. Are you getting better results? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIBrrP-OKHKs",
        "colab_type": "text"
      },
      "source": [
        "**1.12: Removing stop words** What happens if you eliminate the most frequent 50 words (stop words) or 100 words?  Does that improve prediction or make it worse?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI_h3yxKMe2O",
        "colab_type": "text"
      },
      "source": [
        "**1.13: Ignoring absent words** The basic formalism asks you to take account of both present and absent words.  What happens if you ignore the absent words?  Does prediction change?"
      ]
    }
  ]
}